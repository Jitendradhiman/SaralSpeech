# -*- coding: utf-8 -*-
"""timeSeriesPredUsingLSTMV3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EaOTjBrNxQgtwjc9xDWxr60Os_Cf0nob
"""

# Goal:  we will use LSTM cell for predicting a sinusoid
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt

# generate sinusoid
F0, fs, T = 50, 400, 1
L = int(T * fs)
t = np.arange(L) * (1.0/fs)
# generate training data values by adding noise to the original sinusoid
x = np.sin(2 * np.pi * F0 * t) + 0.5 * np.random.randn(L) # you can try changing the SNR and observe the effect on prediction
seq_len = 5 # play with this window size to see the effect on prediction
# for testing
# L = 31
# x = np.arange(L)
batch_size = 3

# Prepare the dataset
features, labels = [], []
for i in range(L - seq_len -1):
  features_temp = x[i:i+seq_len]
  labels_temp = x[i+seq_len]
  features.append(features_temp)
  labels.append(labels_temp)
features = np.array(features)
labels = np.array(labels)

# custum data class  inheriting the properties of PyTorch Dataset class
class dataset(Dataset):
  def __init__(self, features, labels):
    super(dataset, self).__init__()
    self.features = torch.tensor(features, dtype=torch.float32)
    self.labels = torch.tensor(labels, dtype=torch.float32)
  
  def __getitem__(self, idx):
    return self.features[idx], self.labels[idx]
  
  def __len__(self):
    return self.labels.shape[0]

# split the data
def train_test_split(features, labels, split=0.5):
  train_idx = int(split * labels.shape[0])
  X_train = features[:train_idx]
  y_train = labels[:train_idx]
  X_test = features[train_idx:]
  y_test = labels[train_idx:]
  return X_train, y_train, X_test, y_test

# dataloaders
X_train, y_train, X_test, y_test = train_test_split(features, labels)
train_data = dataset(X_train, y_train)
train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_data = dataset(X_test, y_test)
test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)

# define LSTM class
class lstm_pred(nn.Module):
  def __init__(self, input_size, hidden_size, output_size, num_layers=1):
    super(lstm_pred, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)
    self.fc = nn.Linear(hidden_size, output_size)
  
  def forward(self, input):
    state = self.begin_state(input.shape[0])
    lstm_output, state = self.lstm(input, state)
    pred_y = self.fc(lstm_output)
    return pred_y[:, -1, :]
  
  def begin_state(self, batch_size):
    state = (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),
            torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))
    return state

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = lstm_pred(input_size=1, hidden_size=12, output_size=1).to(device)
loss_func = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

def fit(train_data_loader):
    train_epoch_loss = 0.0
    model.train()
    for X, y in train_data_loader:
        X, y = X.to(device), y.to(device)
        pred_y = model(X.view(X.shape[0], X.shape[1], -1))
        loss = loss_func(y, pred_y.flatten())
        train_epoch_loss += loss.item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return train_epoch_loss

def validate(test_loader):
    test_epoch_loss = 0.0
    model.eval()
    for X, y in test_loader:
        X, y = X.to(device), y.to(device)
        pred_y = model(X.view(X.shape[0], X.shape[1], -1))
        loss = loss_func(y, pred_y.flatten())
        test_epoch_loss += loss.item()
        return test_epoch_loss

max_epochs = 101
train_loss, test_loss = [], []
for epoch in range(max_epochs):
  train_epoch_loss = fit(train_data_loader)
  train_loss.append(train_epoch_loss)
  test_epoch_loss = validate(test_data_loader)
  test_loss.append(test_epoch_loss)
  if epoch % 20 == 0:
    print(f'{epoch+1}/{max_epochs}, Training loss: {train_epoch_loss}, Test loss: {test_epoch_loss}')

# predict by using previous predictions (works, practical way of predicting)
preds = []
test_seq = test_data.features[0] # seq_len = 4
new_seq = test_seq
for _ in range(test_data.features.shape[0]):
    # 1 x seq_len --> 1 x seq_len x input_size
    pred = model(new_seq.to(device).view(1, seq_len, -1))
    pred = pred.item()
    preds.append(pred)
    new_seq = torch.cat((new_seq[1:], torch.tensor([pred])), 0)

# predict
test_seq = test_data.features[0] # seq_len = 4
test_pred = model(test_data[:][0].to(device).view(-1, seq_len, 1))

# make plots
plt.figure()
plt.plot(test_data.labels.flatten())
plt.plot(test_pred.detach().cpu().numpy())

# make plots
fig, ax = plt.subplots(4, 1)
ax[0].plot(t, x)
ax[1].plot(test_data.labels.flatten())
ax[1].plot(preds)
ax[2].plot(train_loss, label='training loss')
ax[3].plot(test_loss, label='validation loss')

